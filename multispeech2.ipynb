{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "715bea6e-a17a-4b66-b8bb-6fe181358ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Total samples: 4167\n",
      "ðŸš€ Training on cpu...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [04:49<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Epoch 1: Loss=316.3685 | Train Acc=75.40% | Test Acc=80.82%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [03:11<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Epoch 2: Loss=212.0157 | Train Acc=81.55% | Test Acc=75.78%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [03:13<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Epoch 3: Loss=190.6772 | Train Acc=83.11% | Test Acc=82.61%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [02:41<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Epoch 4: Loss=181.0587 | Train Acc=83.86% | Test Acc=84.41%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [03:01<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Epoch 5: Loss=185.0133 | Train Acc=83.98% | Test Acc=82.73%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [03:14<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Epoch 6: Loss=175.8726 | Train Acc=84.40% | Test Acc=81.29%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [03:15<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Epoch 7: Loss=169.7753 | Train Acc=84.88% | Test Acc=83.69%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [03:05<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Epoch 8: Loss=160.8600 | Train Acc=85.54% | Test Acc=80.70%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [03:15<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Epoch 9: Loss=158.5417 | Train Acc=86.59% | Test Acc=84.29%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [03:02<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Epoch 10: Loss=156.5718 | Train Acc=86.32% | Test Acc=82.25%\n",
      "\n",
      "âœ… Final Test Accuracy: 82.25%\n",
      "\n",
      "ðŸ“‹ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.622     0.220     0.326       127\n",
      "           1      0.810     0.965     0.880       564\n",
      "           2      0.974     0.797     0.877       143\n",
      "\n",
      "    accuracy                          0.823       834\n",
      "   macro avg      0.802     0.661     0.694       834\n",
      "weighted avg      0.809     0.823     0.795       834\n",
      "\n",
      "\n",
      "ðŸ“Š Confusion Matrix:\n",
      "[[ 28  99   0]\n",
      " [ 17 544   3]\n",
      " [  0  29 114]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import librosa\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "N_MFCC = 13\n",
    "MAX_AUDIO_LEN = 100\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "FRAMES_PER_SAMPLE = 10  # we simulate 10 grayscale frames per sample\n",
    "\n",
    "# ========== AUDIO FEATURE ==========\n",
    "\n",
    "def extract_mfcc(wav_path, n_mfcc=N_MFCC, max_len=MAX_AUDIO_LEN):\n",
    "    y, sr = librosa.load(wav_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, max_len - mfcc.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return torch.tensor(mfcc, dtype=torch.float32)\n",
    "\n",
    "# ========== DATASET ==========\n",
    "\n",
    "class MultiModalSpeechDataset(Dataset):\n",
    "    def __init__(self, root_dir, label, transform=None):\n",
    "        self.samples = []\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        for root, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".ult\"):\n",
    "                    base = os.path.splitext(file)[0]\n",
    "                    wav_path = os.path.join(root, base + \".wav\")\n",
    "                    if os.path.exists(wav_path):\n",
    "                        self.samples.append((os.path.join(root, file), wav_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _, wav_path = self.samples[idx]\n",
    "        # Simulate 10 frames of ultrasound as grayscale images\n",
    "        imgs = []\n",
    "        for _ in range(FRAMES_PER_SAMPLE):\n",
    "            frame = Image.fromarray(np.random.randint(0, 255, IMG_SIZE, dtype=np.uint8))\n",
    "            img_tensor = self.transform(frame) if self.transform else transforms.ToTensor()(frame)\n",
    "            imgs.append(img_tensor)\n",
    "        imgs = torch.stack(imgs)  # shape: [10, 1, 128, 128]\n",
    "\n",
    "        mfcc = extract_mfcc(wav_path)\n",
    "        return imgs, mfcc, self.label\n",
    "\n",
    "# ========== MODEL ==========\n",
    "\n",
    "class MultiModalNet(nn.Module):\n",
    "    def __init__(self, audio_feat_dim, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.audio_net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(audio_feat_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32 + 32, 64), nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, ult_imgs, audio_feat):\n",
    "        batch_size, seq_len, c, h, w = ult_imgs.size()\n",
    "        ult_imgs = ult_imgs.view(batch_size * seq_len, c, h, w)\n",
    "        x1 = self.cnn(ult_imgs)  # [batch_size * 10, 32]\n",
    "        x1 = x1.view(batch_size, seq_len, -1).mean(dim=1)  # [batch_size, 32]\n",
    "\n",
    "        x2 = self.audio_net(audio_feat)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ========== TRAINING SCRIPT ==========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # ðŸ” Load datasets\n",
    "    dataset_uxtd = MultiModalSpeechDataset(\"D:/UltraSuite/core-uxtd/core\", label=0, transform=transform)\n",
    "    dataset_uxssd = MultiModalSpeechDataset(\"D:/UltraSuite/core-uxssd/core\", label=1, transform=transform)\n",
    "    dataset_upx   = MultiModalSpeechDataset(\"D:/UltraSuite/core-upx/core\", label=2, transform=transform)\n",
    "\n",
    "    full_dataset = dataset_uxtd + dataset_uxssd + dataset_upx\n",
    "    print(f\"ðŸ“¦ Total samples: {len(full_dataset)}\")\n",
    "\n",
    "    # âœ‚ Split\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_data, test_data = random_split(full_dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # ðŸš€ Model Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MultiModalNet(audio_feat_dim=N_MFCC * MAX_AUDIO_LEN).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print(f\"ðŸš€ Training on {device}...\\n\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for imgs, mfccs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            imgs, mfccs, labels = imgs.to(device), mfccs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs, mfccs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        # Test set\n",
    "        model.eval()\n",
    "        test_correct, test_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, mfccs, labels in test_loader:\n",
    "                imgs, mfccs, labels = imgs.to(device), mfccs.to(device), labels.to(device)\n",
    "                outputs = model(imgs, mfccs)\n",
    "                test_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "        test_acc = 100 * test_correct / test_total\n",
    "\n",
    "        print(f\"ðŸ“Š Epoch {epoch+1}: Loss={total_loss:.4f} | Train Acc={train_acc:.2f}% | Test Acc={test_acc:.2f}%\\n\")\n",
    "\n",
    "    # ðŸ“ˆ Final Evaluation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, mfccs, labels in test_loader:\n",
    "            imgs, mfccs = imgs.to(device), mfccs.to(device)\n",
    "            outputs = model(imgs, mfccs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    print(f\"âœ… Final Test Accuracy: {accuracy_score(all_labels, all_preds) * 100:.2f}%\")\n",
    "    print(\"\\nðŸ“‹ Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, digits=3))\n",
    "    print(\"\\nðŸ“Š Confusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
