# ğŸ™ï¸ MultiSpeech: Multimodal Detection of Speech Sound Disorders from Ultrasound & Audio

This project builds a **deep learning pipeline** that classifies speech data into three categories:

- **Typically Developing (Control)** â€“ UXTD
- **Speech Sound Disorders (SSD)** â€“ UXSSD
- **Clinical/Varied Cases** â€“ UPX

We use both **ultrasound tongue images (.ult)** and **audio recordings (.wav)**, leveraging **multimodal learning** to improve classification performance.

---

## ğŸ“‚ Dataset: [UltraSuite](https://ultrasuite.github.io/)

We use data from the UltraSuite repository, which contains synchronized:

- `.ult` â€” raw ultrasound images of the tongue during speech
- `.wav` â€” audio recordings
- `.txt` â€” utterance prompts
- `.param` â€” ultrasound metadata

### ğŸ§  Classes Used:
| Class | Dataset | Label |
|-------|---------|-------|
| Control | UXTD | 0 |
| Speech Sound Disorder | UXSSD | 1 |
| Clinical Mixed | UPX | 2 |

We sampled from **5 speakers per dataset** for a balanced training set.

> **To use this project**, download the data directly via [UltraSuite](https://ultrasuite.github.io/download_win/) using `rsync` or their zipped samples.

---

## ğŸ§  Why Multimodal?

Ultrasound captures **articulatory movement**, and audio captures **acoustic output**. Combining both offers richer signals, improving generalization for speech diagnostics.

---

## ğŸ” Pipeline Flowchart

[ Ultrasound (.ult) ]        [ Audio (.wav) ]
          â†“                         â†“
  Simulated Image Frame       MFCC Extraction
          â†“                         â†“
       CNN Layer              Feedforward Network
               â†˜             â†™
               [ Concatenation ]
                      â†“
               [ Classifier ]
                      â†“
     Output â†’ [Control | SSD | Clinical]

---

## ğŸ§ª Results

On ~4,100 multimodal samples:

- **Best Accuracy**: 86%
- **Balanced Accuracy** (with 5 speakers each): ~79%
- **Model**: CNN for image + Dense layers for audio features

Example confusion matrix:
```
[ [TP_0, FP_0, FN_0],
  [FP_1, TP_1, FN_1],
  [FP_2, FN_2, TP_2] ]
```

---

## ğŸš€ Quickstart

1. **Install requirements**
   ```bash
   pip install -r requirements.txt
   ```

2. **Download Data**
   - Use `rsync` or download from: https://ultrasuite.github.io/download_win/

3. **Run Training**
   ```bash
   python main.py
   ```

---

## ğŸ“ Folder Structure

```
.
â”œâ”€â”€ main.py               # Main training + evaluation script
â”œâ”€â”€ data_loader.py        # Custom PyTorch Dataset
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```

---

## ğŸ§  What We Did

- Designed a **CNN + Dense multimodal architecture**
- Simulated `.ult` files as grayscale images
- Extracted **MFCCs from `.wav`** files using librosa
- Performed **random split** with ~80/20 train/test
- Balanced samples across classes (UXTD, UXSSD, UPX)
- Evaluated using accuracy, precision, recall, F1-score, confusion matrix

---

## ğŸ”¬ Why This Matters

- Can aid in **early screening** of speech disorders
- Offers a **non-invasive, low-cost diagnostic aid**
- Potential for use in **low-resource clinics** or tele-therapy setups

---



## ğŸ“œ License

MIT License â€” Open for research and non-commercial use.