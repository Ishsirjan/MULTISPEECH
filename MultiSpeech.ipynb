{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c0aabd4-734e-4f86-ad44-a806955581bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Total samples: 4167\n",
      "🚀 Training on cpu...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 417/417 [01:09<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1: Loss=282.0607 | Train Acc=76.03% | Test Acc=80.94%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 417/417 [01:07<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 2: Loss=225.6155 | Train Acc=80.98% | Test Acc=81.06%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 417/417 [01:04<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 3: Loss=209.9828 | Train Acc=81.40% | Test Acc=80.70%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 417/417 [01:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 4: Loss=205.6538 | Train Acc=81.85% | Test Acc=81.06%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 417/417 [01:02<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 5: Loss=197.3443 | Train Acc=82.18% | Test Acc=80.58%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 417/417 [01:03<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 6: Loss=194.8964 | Train Acc=82.36% | Test Acc=81.06%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 417/417 [01:17<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 7: Loss=190.4005 | Train Acc=82.33% | Test Acc=77.10%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 417/417 [01:18<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 8: Loss=185.2120 | Train Acc=82.36% | Test Acc=80.94%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 417/417 [01:20<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 9: Loss=185.1583 | Train Acc=82.45% | Test Acc=80.82%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 417/417 [01:15<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 10: Loss=178.9655 | Train Acc=82.90% | Test Acc=79.38%\n",
      "\n",
      "✅ Final Test Accuracy: 79.38%\n",
      "\n",
      "📋 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.455     0.037     0.068       135\n",
      "           1      0.787     0.943     0.858       546\n",
      "           2      0.840     0.928     0.882       153\n",
      "\n",
      "    accuracy                          0.794       834\n",
      "   macro avg      0.694     0.636     0.603       834\n",
      "weighted avg      0.743     0.794     0.735       834\n",
      "\n",
      "\n",
      "📊 Confusion Matrix:\n",
      "[[  5 128   2]\n",
      " [  6 515  25]\n",
      " [  0  11 142]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import librosa\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "N_MFCC = 13\n",
    "MAX_AUDIO_LEN = 100\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "\n",
    "# ========== AUDIO FEATURE ==========\n",
    "def extract_mfcc(wav_path, n_mfcc=N_MFCC, max_len=MAX_AUDIO_LEN):\n",
    "    y, sr = librosa.load(wav_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, max_len - mfcc.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return torch.tensor(mfcc, dtype=torch.float32)\n",
    "\n",
    "# ========== DATASET ==========\n",
    "class MultiModalSpeechDataset(Dataset):\n",
    "    def __init__(self, root_dir, label, transform=None):\n",
    "        self.samples = []\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        for root, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".ult\"):\n",
    "                    base = os.path.splitext(file)[0]\n",
    "                    wav_path = os.path.join(root, base + \".wav\")\n",
    "                    if os.path.exists(wav_path):\n",
    "                        self.samples.append((os.path.join(root, file), wav_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ult_path, wav_path = self.samples[idx]\n",
    "        img = Image.fromarray(np.random.randint(0, 255, IMG_SIZE, dtype=np.uint8))\n",
    "        img = self.transform(img) if self.transform else transforms.ToTensor()(img)\n",
    "        mfcc = extract_mfcc(wav_path)\n",
    "        return img, mfcc, self.label\n",
    "\n",
    "# ========== MODEL ==========\n",
    "class MultiModalNet(nn.Module):\n",
    "    def __init__(self, audio_feat_dim, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.audio_net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(audio_feat_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32 + 32, 64), nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, ult_img, audio_feat):\n",
    "        x1 = self.cnn(ult_img)\n",
    "        x2 = self.audio_net(audio_feat)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ========== TRAINING SCRIPT ==========\n",
    "if __name__ == \"__main__\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # 🔍 Load datasets\n",
    "    dataset_uxtd = MultiModalSpeechDataset(\"D:/UltraSuite/core-uxtd/core\", label=0, transform=transform)\n",
    "    dataset_uxssd = MultiModalSpeechDataset(\"D:/UltraSuite/core-uxssd/core\", label=1, transform=transform)\n",
    "    dataset_upx   = MultiModalSpeechDataset(\"D:/UltraSuite/core-upx/core\", label=2, transform=transform)\n",
    "\n",
    "    full_dataset = dataset_uxtd + dataset_uxssd + dataset_upx\n",
    "    print(f\"📦 Total samples: {len(full_dataset)}\")\n",
    "\n",
    "    # ✂ Split\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_data, test_data = random_split(full_dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # 🚀 Model Setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MultiModalNet(audio_feat_dim=N_MFCC * MAX_AUDIO_LEN).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print(f\"🚀 Training on {device}...\\n\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for imgs, mfccs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            imgs, mfccs, labels = imgs.to(device), mfccs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs, mfccs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        # ========== Evaluate on Test Set ==========\n",
    "        model.eval()\n",
    "        test_correct, test_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, mfccs, labels in test_loader:\n",
    "                imgs, mfccs, labels = imgs.to(device), mfccs.to(device), labels.to(device)\n",
    "                outputs = model(imgs, mfccs)\n",
    "                test_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "        test_acc = 100 * test_correct / test_total\n",
    "\n",
    "        print(f\"📊 Epoch {epoch+1}: Loss={total_loss:.4f} | Train Acc={train_acc:.2f}% | Test Acc={test_acc:.2f}%\\n\")\n",
    "\n",
    "    # 🧪 Final Evaluation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, mfccs, labels in test_loader:\n",
    "            imgs, mfccs = imgs.to(device), mfccs.to(device)\n",
    "            outputs = model(imgs, mfccs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    print(f\"✅ Final Test Accuracy: {accuracy_score(all_labels, all_preds) * 100:.2f}%\")\n",
    "    print(\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, digits=3))\n",
    "    print(\"\\n📊 Confusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba9750fa-2673-4e25-9cad-a4ca00f72a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Total samples: 4167\n",
      "🚀 Training on cpu...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 417/417 [01:17<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=336.9858 | Train Acc=72.49% | Test Acc=80.10%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 417/417 [01:16<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=236.7455 | Train Acc=79.60% | Test Acc=83.81%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 417/417 [01:12<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=208.0756 | Train Acc=81.43% | Test Acc=84.17%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 417/417 [01:03<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=205.6213 | Train Acc=81.43% | Test Acc=83.81%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 417/417 [01:05<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=196.4777 | Train Acc=82.48% | Test Acc=85.25%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 417/417 [01:17<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss=173.6596 | Train Acc=84.22% | Test Acc=84.65%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 417/417 [01:14<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss=174.5874 | Train Acc=84.43% | Test Acc=85.61%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 417/417 [01:14<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss=175.8236 | Train Acc=84.52% | Test Acc=84.53%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 417/417 [01:14<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss=173.1142 | Train Acc=84.67% | Test Acc=77.82%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 417/417 [01:15<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss=164.1296 | Train Acc=85.42% | Test Acc=82.97%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 417/417 [01:04<00:00,  6.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss=154.9363 | Train Acc=86.17% | Test Acc=84.41%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 417/417 [01:02<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss=146.2685 | Train Acc=86.68% | Test Acc=84.41%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 417/417 [01:03<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss=144.6661 | Train Acc=87.01% | Test Acc=85.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 417/417 [01:02<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss=146.4268 | Train Acc=86.89% | Test Acc=82.13%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 417/417 [01:02<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss=139.7864 | Train Acc=87.13% | Test Acc=83.09%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 417/417 [01:03<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Loss=131.6991 | Train Acc=87.79% | Test Acc=84.53%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 417/417 [01:02<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Loss=130.9026 | Train Acc=88.06% | Test Acc=84.29%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 417/417 [01:02<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Loss=127.5266 | Train Acc=88.51% | Test Acc=85.13%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 417/417 [01:02<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Loss=128.0576 | Train Acc=89.02% | Test Acc=84.29%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 417/417 [01:02<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss=124.2887 | Train Acc=88.81% | Test Acc=84.05%\n",
      "\n",
      "✅ Final Test Accuracy: 84.05%\n",
      "\n",
      "📋 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.505     0.446     0.474       112\n",
      "           1      0.875     0.905     0.890       588\n",
      "           2      0.937     0.888     0.912       134\n",
      "\n",
      "    accuracy                          0.841       834\n",
      "   macro avg      0.772     0.746     0.758       834\n",
      "weighted avg      0.835     0.841     0.837       834\n",
      "\n",
      "\n",
      "📊 Confusion Matrix:\n",
      "[[ 50  62   0]\n",
      " [ 48 532   8]\n",
      " [  1  14 119]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import librosa\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "N_MFCC = 13\n",
    "MAX_AUDIO_LEN = 100\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# ========== AUDIO FEATURE ==========\n",
    "def extract_mfcc(wav_path, n_mfcc=N_MFCC, max_len=MAX_AUDIO_LEN):\n",
    "    y, sr = librosa.load(wav_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    if mfcc.shape[1] < max_len:\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, max_len - mfcc.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    return torch.tensor(mfcc, dtype=torch.float32)\n",
    "\n",
    "# ========== DATASET ==========\n",
    "class MultiModalSpeechDataset(Dataset):\n",
    "    def __init__(self, root_dir, label, transform=None):\n",
    "        self.samples = []\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        for root, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".ult\"):\n",
    "                    base = os.path.splitext(file)[0]\n",
    "                    wav_path = os.path.join(root, base + \".wav\")\n",
    "                    if os.path.exists(wav_path):\n",
    "                        self.samples.append((os.path.join(root, file), wav_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ult_path, wav_path = self.samples[idx]\n",
    "        img = Image.fromarray(np.random.randint(0, 255, IMG_SIZE, dtype=np.uint8))\n",
    "        img = self.transform(img) if self.transform else transforms.ToTensor()(img)\n",
    "        mfcc = extract_mfcc(wav_path)\n",
    "        return img, mfcc, self.label\n",
    "\n",
    "# ========== MODEL ==========\n",
    "class MultiModalNet(nn.Module):\n",
    "    def __init__(self, audio_feat_dim, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(16), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.BatchNorm2d(32), nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.audio_net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(audio_feat_dim, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32 + 32, 64), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, ult_img, audio_feat):\n",
    "        x1 = self.cnn(ult_img)\n",
    "        x2 = self.audio_net(audio_feat)\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ========== TRAINING ==========\n",
    "if __name__ == \"__main__\":\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset_uxtd = MultiModalSpeechDataset(\"D:/UltraSuite/core-uxtd/core\", label=0, transform=transform)\n",
    "    dataset_uxssd = MultiModalSpeechDataset(\"D:/UltraSuite/core-uxssd/core\", label=1, transform=transform)\n",
    "    dataset_upx   = MultiModalSpeechDataset(\"D:/UltraSuite/core-upx/core\", label=2, transform=transform)\n",
    "\n",
    "    full_dataset = dataset_uxtd + dataset_uxssd + dataset_upx\n",
    "    print(f\"📦 Total samples: {len(full_dataset)}\")\n",
    "\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_data, test_data = random_split(full_dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MultiModalNet(audio_feat_dim=N_MFCC * MAX_AUDIO_LEN).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "    print(f\"🚀 Training on {device}...\\n\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for imgs, mfccs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            imgs, mfccs, labels = imgs.to(device), mfccs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs, mfccs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        # Evaluation on test set\n",
    "        model.eval()\n",
    "        test_correct, test_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, mfccs, labels in test_loader:\n",
    "                imgs, mfccs, labels = imgs.to(device), mfccs.to(device), labels.to(device)\n",
    "                outputs = model(imgs, mfccs)\n",
    "                test_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "        test_acc = 100 * test_correct / test_total\n",
    "        print(f\"Epoch {epoch+1}: Loss={total_loss:.4f} | Train Acc={train_acc:.2f}% | Test Acc={test_acc:.2f}%\\n\")\n",
    "        scheduler.step()\n",
    "\n",
    "    # 🧪 Final Evaluation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, mfccs, labels in test_loader:\n",
    "            imgs, mfccs = imgs.to(device), mfccs.to(device)\n",
    "            outputs = model(imgs, mfccs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    print(f\"✅ Final Test Accuracy: {accuracy_score(all_labels, all_preds) * 100:.2f}%\")\n",
    "    print(\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, digits=3))\n",
    "    print(\"\\n📊 Confusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
